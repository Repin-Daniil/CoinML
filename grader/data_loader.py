"""
–ú–æ–¥—É–ª—å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –º–æ–Ω–µ—Ç
"""
import os
from pathlib import Path
import tensorflow as tf
import numpy as np


from pathlib import Path

def scan_dataset(data_path, banned_file="banned.txt"):
    """
    –°–∫–∞–Ω–∏—Ä—É–µ—Ç –ø–∞–ø–∫—É —Å –¥–∞–Ω–Ω—ã–º–∏ –∏ —Å–æ–∑–¥–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –º–µ—Ç–∫–∞–º–∏.
    –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –º–æ–Ω–µ—Ç—ã, ID –∫–æ—Ç–æ—Ä—ã—Ö —É–∫–∞–∑–∞–Ω—ã –≤ banned.txt.

    Args:
        data_path (str): –ü—É—Ç—å –∫ –∫–æ—Ä–Ω–µ–≤–æ–π –ø–∞–ø–∫–µ —Å –¥–∞–Ω–Ω—ã–º–∏
        banned_file (str): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å–æ —Å–ø–∏—Å–∫–æ–º –∑–∞–±–∞–Ω–µ–Ω–Ω—ã—Ö ID (–ø–æ –æ–¥–Ω–æ–º—É –≤ —Å—Ç—Ä–æ–∫–µ)

    Returns:
        list: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (obverse_path, reverse_path, label_index)
    """
    data_path = Path(data_path)
    samples = []

    # --- –ß–∏—Ç–∞–µ–º —Å–ø–∏—Å–æ–∫ –∑–∞–±–∞–Ω–µ–Ω–Ω—ã—Ö coin_id ---
    banned_path = Path(banned_file)
    banned_ids = set()
    if banned_path.exists():
        with open(banned_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line:
                    banned_ids.add(line)
        print(f"üõë –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(banned_ids)} –∑–∞–±–∞–Ω–µ–Ω–Ω—ã—Ö ID –∏–∑ {banned_file}")
    else:
        print(f"‚ö†Ô∏è –§–∞–π–ª {banned_file} –Ω–µ –Ω–∞–π–¥–µ–Ω, —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è")

    # --- –û–±—Ö–æ–¥ –∫–ª–∞—Å—Å–æ–≤ ---
    for class_folder in sorted(data_path.iterdir()):
        if not class_folder.is_dir():
            continue

        try:
            label = int(class_folder.name)
            if label < 1 or label > 5:
                continue
        except ValueError:
            continue

        label_index = label - 1
        all_files = {f.stem: f for f in class_folder.glob("*.jpg")}

        coin_ids = set()
        for filename in all_files.keys():
            if '_obverse' in filename:
                coin_id = filename.replace('_obverse', '')
                coin_ids.add(coin_id)
            elif '_reverse' in filename:
                coin_id = filename.replace('_reverse', '')
                coin_ids.add(coin_id)

        for coin_id in coin_ids:
            # --- –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–∞–Ω ---
            if coin_id in banned_ids:
                print(f"üö´ –ü—Ä–æ–ø—É—â–µ–Ω –∑–∞–±–∞–Ω–µ–Ω–Ω—ã–π coin_id: {coin_id}")
                continue

            obverse_name = f"{coin_id}_obverse"
            reverse_name = f"{coin_id}_reverse"

            if obverse_name in all_files and reverse_name in all_files:
                obverse_path = str(all_files[obverse_name])
                reverse_path = str(all_files[reverse_name])
                samples.append((obverse_path, reverse_path, label_index))

    print(f"‚úì –ù–∞–π–¥–µ–Ω–æ {len(samples)} –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –º–æ–Ω–µ—Ç –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")

    # --- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º ---
    class_counts = {}
    for _, _, label in samples:
        class_counts[label] = class_counts.get(label, 0) + 1

    print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Å–∞–º:")
    for label in sorted(class_counts.keys()):
        print(f"  –ö–ª–∞—Å—Å {label+1}: {class_counts[label]} –æ–±—Ä–∞–∑—Ü–æ–≤")

    return samples



def create_augmentation_layer(config):
    """
    –°–æ–∑–¥–∞–µ—Ç Sequential —Å–ª–æ–π —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥–∞
    
    Args:
        config (dict): –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        
    Returns:
        tf.keras.Sequential: –°–ª–æ–π —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è–º–∏
    """
    aug_config = config['augmentation']
    
    layers = []
    
    # –§–ª–∏–ø—ã
    if aug_config['flip_mode'] == 'horizontal':
        layers.append(tf.keras.layers.RandomFlip("horizontal"))
    elif aug_config['flip_mode'] == 'vertical':
        layers.append(tf.keras.layers.RandomFlip("vertical"))
    elif aug_config['flip_mode'] == 'horizontal_and_vertical':
        layers.append(tf.keras.layers.RandomFlip("horizontal_and_vertical"))
    
    # –ü–æ–≤–æ—Ä–æ—Ç
    if aug_config['rotation_factor'] > 0:
        layers.append(tf.keras.layers.RandomRotation(aug_config['rotation_factor']))
    
    # Zoom
    if aug_config.get('zoom_factor', 0) > 0:
        layers.append(tf.keras.layers.RandomZoom(aug_config['zoom_factor']))
    
    # –Ø—Ä–∫–æ—Å—Ç—å
    if aug_config['brightness_factor'] > 0:
        layers.append(tf.keras.layers.RandomBrightness(aug_config['brightness_factor']))
    
    return tf.keras.Sequential(layers, name='augmentation')


def load_and_preprocess_image(path, image_size):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    
    Args:
        path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
        image_size (int): –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        
    Returns:
        tf.Tensor: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ [H, W, 3]
    """
    # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
    img = tf.io.read_file(path)
    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º JPEG
    img = tf.image.decode_jpeg(img, channels=3)
    # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä
    img = tf.image.resize(img, [image_size, image_size])
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ [0, 1]
    img = img / 255.0
    
    return img


def create_dataset_pipeline(samples, config, is_training=True):
    """
    –°–æ–∑–¥–∞–µ—Ç tf.data.Dataset –ø–∞–π–ø–ª–∞–π–Ω
    
    Args:
        samples (list): –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (obverse_path, reverse_path, label)
        config (dict): –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        is_training (bool): –§–ª–∞–≥ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ (–¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏)
        
    Returns:
        tf.data.Dataset: –ì–æ—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    """
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —Å–ø–∏—Å–∫–∏
    obverse_paths = [s[0] for s in samples]
    reverse_paths = [s[1] for s in samples]
    labels = [s[2] for s in samples]
    
    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ –ø—É—Ç–µ–π
    dataset = tf.data.Dataset.from_tensor_slices(
        (obverse_paths, reverse_paths, labels)
    )
    
    image_size = config['data']['image_size']
    
    # –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–∞—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    def load_pair(obverse_path, reverse_path, label):
        img_a = load_and_preprocess_image(obverse_path, image_size)
        img_b = load_and_preprocess_image(reverse_path, image_size)
        return (img_a, img_b), label
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∑–∞–≥—Ä—É–∑–∫—É
    dataset = dataset.map(
        load_pair,
        num_parallel_calls=tf.data.AUTOTUNE
    )
    
    # –ö—ç—à–∏—Ä—É–µ–º (—Ç.–∫. –¥–∞–Ω–Ω—ã–µ –ø–æ–º–µ—Å—Ç—è—Ç—Å—è –≤ RAM)
    dataset = dataset.cache()
    
    # Shuffle —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏
    if is_training:
        dataset = dataset.shuffle(buffer_size=len(samples))
    
    # –ë–∞—Ç—á–∏—Ä–æ–≤–∞–Ω–∏–µ
    batch_size = config['training']['batch_size']
    dataset = dataset.batch(batch_size)
    
    # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è (–ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ –±–∞—Ç—á—É, —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è)
    if is_training:
        augmentation = create_augmentation_layer(config)
        
        def apply_augmentation(images, labels):
            img_a, img_b = images
            img_a = augmentation(img_a, training=True)
            img_b = augmentation(img_b, training=True)
            return (img_a, img_b), labels
        
        dataset = dataset.map(
            apply_augmentation,
            num_parallel_calls=tf.data.AUTOTUNE
        )
    
    # Prefetch –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    
    return dataset


def get_datasets(config):
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ–±—É—á–∞—é—â–µ–π –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–æ–∫
    
    Args:
        config (dict): –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞
        
    Returns:
        tuple: (train_dataset, val_dataset)
    """
    print("\n" + "="*50)
    print("–ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•")
    print("="*50)
    
    # –°–∫–∞–Ω–∏—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    samples = scan_dataset(config['data']['path'])
    
    if len(samples) == 0:
        raise ValueError("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π –ø–∞—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π!")
    
    # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º
    np.random.shuffle(samples)
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val
    val_split = config['data']['val_split']
    split_idx = int(len(samples) * (1 - val_split))
    
    train_samples = samples[:split_idx]
    val_samples = samples[split_idx:]
    
    print(f"\n–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
    print(f"  –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(train_samples)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"  –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(val_samples)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω—ã
    train_dataset = create_dataset_pipeline(train_samples, config, is_training=True)
    val_dataset = create_dataset_pipeline(val_samples, config, is_training=False)
    
    print(f"\n‚úì –î–∞—Ç–∞—Å–µ—Ç—ã —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω—ã")
    print("="*50 + "\n")
    
    return train_dataset, val_dataset